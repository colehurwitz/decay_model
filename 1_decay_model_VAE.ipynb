{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the VAE to localize\n",
    "\n",
    "This notebook shows how the VAE is trained to localize spikes in a recording for the paper: *Scalable Spike Source Localization in Extracellular Recordings using Amortized Variational Inference*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "#My modules\n",
    "from utility_functions import vae_functions\n",
    "from models import EXPVAEWAVE\n",
    "\n",
    "#General modules\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import h5py\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import os\n",
    "from collections import namedtuple, defaultdict\n",
    "import MEArec as mr\n",
    "import spikeextractors as se\n",
    "from utility_functions import clustering_plotting_functions\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "print('torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())\n",
    "\n",
    "TrainSpike = namedtuple('DataPoint', ['amps', 'waveforms', 'ch_locs', 'center_loc', 'spike_id', 'exp_id',\\\n",
    "                                      'min_waveform', 'min_amp'])\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rc('xtick', labelsize=12) \n",
    "mpl.rc('ytick', labelsize=12) \n",
    "mpl.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Here we set the parameters for the VAE, including epochs, batch size, etc. These parameters seem to work well across multiple datasets so they don't necessarily need to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='VAE Decay Model')\n",
    "parser.add_argument('--batch-size', type=int, default=50, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=200, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 0)')\n",
    "parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--learning-rate', type=float, default=1e-3, metavar='N',\n",
    "                    help='learning rate of ADAM')\n",
    "parser.add_argument('--beta-one', type=float, default=0.99, metavar='N',\n",
    "                    help='beta one ADAM')\n",
    "parser.add_argument('--beta-two', type=float, default=0.999, metavar='N',\n",
    "                    help='beta two ADAM')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "if(args.cuda):\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the augmented training data and also the labels (provided for ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_directory = '/disk/scratch/cole/recordings/'\n",
    "augmented_data_path = '/disk/scratch/cole/recordings/'\n",
    "recording_name = 'recordings_300_SqMEA-10-15um_minamp0_60s_10uV_far-neurons_bpf_25-03-2019.h5'\n",
    "width = 40\n",
    "amp_jitter = 0\n",
    "\n",
    "recgen = mr.load_recordings(recording_directory + recording_name)\n",
    "channel_positions = recgen.channel_positions\n",
    "soma_positions = np.asarray([st.annotations['soma_position'] for st in recgen.spiketrains])\n",
    "SX_groundtruth = se.MEArecSortingExtractor(recording_directory + recording_name)\n",
    "\n",
    "hf_AO = h5py.File(str(augmented_data_path) + 'model_data_gt_'+ str(width) + 'um_VAE_'+str(amp_jitter)+'_amp_jitter_' + str(recording_name), 'r')\n",
    "amp_array = np.asarray(hf_AO['amps_list'])\n",
    "channel_loc_array = np.asarray(hf_AO['channel_locations_list'])\n",
    "waveforms_array = np.asarray(hf_AO['waveforms_list'])\n",
    "center_loc_array = np.asarray(hf_AO['center_location_list'])\n",
    "central_channel_list = np.asarray(hf_AO['central_channel_list'])\n",
    "spike_time_list = np.asarray(hf_AO['spike_time_list'])\n",
    "spike_id_list = np.asarray(hf_AO['spike_id_list'])\n",
    "hf_AO.close()\n",
    "\n",
    "hf_label_AO = h5py.File(str(augmented_data_path) + 'label_data_gt_' + str(width) + 'um_VAE_'+str(amp_jitter)+'_amp_jitter_' + str(recording_name), 'r')\n",
    "neuron_loc_array = np.asarray(hf_label_AO['neuron_locations_list'])\n",
    "neuron_array = np.asarray(hf_label_AO['neuron_list'])\n",
    "overlap_array = np.asarray(hf_label_AO['overlap_list'])\n",
    "hf_label_AO.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now choose the waveform shape for the input, augment the waveforms to indicate whether they come from real or virtual channels, extract the min waveforms for each event, and then\n",
    "load the augmented data into a pytorch dataloader. This can be directly passed into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random seeds\n",
    "torch.manual_seed(0)\n",
    "mid_frame = waveforms_array.shape[2]//2\n",
    "cutout_start = 30\n",
    "cutout_end = 30\n",
    "\n",
    "\n",
    "#Augment the waveforms to indicate which come from real or virtual channels\n",
    "waveforms_array = waveforms_array[:,:,mid_frame-cutout_start:mid_frame+cutout_start]\n",
    "waveforms_array = np.concatenate((waveforms_array, amp_array[:,:,1].reshape(amp_array.shape[0], amp_array.shape[1], 1)),axis=2)\n",
    "\n",
    "num_spikes = waveforms_array.shape[0]\n",
    "num_amps = amp_array[0].shape[0]\n",
    "\n",
    "torch_train_amps = torch.from_numpy(np.asarray(amp_array)).float()\n",
    "torch_train_waveforms = torch.from_numpy(np.asarray(waveforms_array)).float()\n",
    "torch_train_ch_locs = torch.from_numpy(np.asarray(channel_loc_array)).float()\n",
    "exp_indices = torch.from_numpy(np.asarray(range(num_spikes)))\n",
    "\n",
    "min_waveforms_list = []\n",
    "for event_idx in range(amp_array.shape[0]):\n",
    "    min_waveforms_list.append(waveforms_array[event_idx][np.argsort(amp_array[event_idx], 0)[:,0]][0][:-1])\n",
    "min_waveforms_list = np.asarray(min_waveforms_list)\n",
    "\n",
    "train_spikes = []\n",
    "for i, amps in enumerate(torch_train_amps):\n",
    "    waveforms = torch_train_waveforms[i]\n",
    "    channel_locs = torch_train_ch_locs[i]\n",
    "    center_loc = center_loc_array[i]\n",
    "    exp_id = exp_indices[i]\n",
    "    spike_id = spike_id_list[i]\n",
    "    min_amp = np.min(min_waveforms_list[i])\n",
    "    min_waveform = torch.from_numpy(min_waveforms_list[i]).float()\n",
    "    train_spike = TrainSpike(amps=amps, waveforms=waveforms, ch_locs=channel_locs, center_loc=center_loc, spike_id=spike_id, exp_id=exp_id, \\\n",
    "                             min_waveform=min_waveform, min_amp=min_amp)\n",
    "    train_spikes.append(train_spike)\n",
    "\n",
    "training_set = vae_functions.EventDataset(train_spikes)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "print(\"Dataset: \" + str(len(training_set)))\n",
    "print(\"Waveform Input Size: \" + str(training_set[0][1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we initialize the exponential parameters for the model according to the manuscript and then train the model (called EXPVAEWAVE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = 2.0\n",
    "abs_ = np.zeros((num_spikes, 2))\n",
    "#Set exponential parameter initial values\n",
    "for i, training_element in enumerate(training_set):\n",
    "    init_peak = math.log(-(training_element[6].item()*scalar))\n",
    "    abs_[i] = np.asarray([init_peak, -0.035])\n",
    "\n",
    "model = EXPVAEWAVE.EXPVAEWAVE(training_set, args, abs_, optimize_both_exp=False, batchnorm=True, prior_var = 80)\n",
    "model = model.to(device)\n",
    "model.apply(EXPVAEWAVE.weight_init)\n",
    "optimizer = optim.Adam(list(model.parameters()) + [model.exps], lr=args.learning_rate, weight_decay=0,\n",
    "                            betas=(args.beta_one, args.beta_two))\n",
    "\n",
    "train_losses = []\n",
    "n_epochs = args.epochs\n",
    "mb = master_bar(range(20))\n",
    "y_ax_index= 0 \n",
    "for i in mb:\n",
    "    epoch = i\n",
    "    for j in progress_bar(range(int(n_epochs/20)), parent=mb):\n",
    "        model = EXPVAEWAVE.train(model, device, args, optimizer, train_loader, epoch, train_losses)\n",
    "    x = range(len(train_losses))\n",
    "    y = train_losses\n",
    "    graphs = [[x,y]]\n",
    "    y_bounds = [0,train_losses[0]]\n",
    "    mb.update_graph(graphs, y_bounds=y_bounds)\n",
    "    mb.write(f'Avg. Training Loss:  {train_losses[-1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the location estimates for each spike in the recording, we run them through the inference network and then average the location estimates belonging to the same event (this is described in the manuscript in the amplitude jitter portion of the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = vae_functions.EventDataset(train_spikes)\n",
    "eval_loader = torch.utils.data.DataLoader(eval_set, batch_size=args.batch_size, shuffle=False)\n",
    "model.eval()\n",
    "center_locs = []\n",
    "vae_locs = []\n",
    "spike_ids_list = []\n",
    "for batch_idx, (t_amps, t_waveforms, t_ch_locs, center_loc, spike_ids, _, _, _) in enumerate(eval_loader):\n",
    "    t_amps = t_amps.to(device) #torch.Size([200, 49, 2])\n",
    "    t_waveforms = t_waveforms.to(device)\n",
    "    x_mu, x_var, y_mu, y_var, z_mu, z_var = model.encode(t_amps, t_waveforms)\n",
    "    vae_locs.append(np.asarray([x_mu.cpu().detach().numpy().squeeze(), y_mu.cpu().detach().numpy().squeeze() + center_loc[:,1].numpy(), z_mu.cpu().detach().numpy().squeeze() + center_loc[:,2].numpy()]))\n",
    "    center_locs.append(np.asarray([center_loc[:,0].numpy(), center_loc[:,1].numpy(), center_loc[:,2].numpy()]))\n",
    "    spike_ids_list.append(np.asarray(spike_ids))\n",
    "vae_locs_all = np.concatenate(vae_locs, axis=1).T\n",
    "center_locs_all = np.concatenate(center_locs, axis=1).T\n",
    "spike_ids_array = np.concatenate(spike_ids_list, axis=0)\n",
    "\n",
    "idx_sort = np.argsort(spike_ids_array)\n",
    "sorted_spike_ids_array = spike_ids_array[idx_sort]\n",
    "vals, idx_start, count = np.unique(sorted_spike_ids_array, return_counts=True,\n",
    "                                return_index=True)\n",
    "res = np.split(idx_sort, idx_start[1:])\n",
    "#filter them with respect to their size, keeping only items occurring more than once\n",
    "vals = vals[count >= 1]\n",
    "res = filter(lambda x: x.size >= 1, res)\n",
    "dup_spike_ids = []\n",
    "for r in res:\n",
    "    dup_spike_ids.append(r)\n",
    "    \n",
    "averaged_vae_locs = []\n",
    "averaged_spike_times = []\n",
    "averaged_min_waveforms = []\n",
    "curr_index = 0\n",
    "for spike_ids in dup_spike_ids:\n",
    "    vae_loc = np.zeros(3)\n",
    "    for spike_id in spike_ids:\n",
    "        vae_loc[0] += vae_locs_all[curr_index][0]\n",
    "        vae_loc[1] += vae_locs_all[curr_index][1]\n",
    "        vae_loc[2] += vae_locs_all[curr_index][2]\n",
    "        curr_index += 1\n",
    "    vae_loc = vae_loc/len(spike_ids)\n",
    "    averaged_vae_locs.append(vae_loc)\n",
    "    averaged_spike_times.append(spike_time_list[spike_ids[0]])\n",
    "    averaged_min_waveforms.append(min_waveforms_list[spike_ids[0]])\n",
    "    \n",
    "averaged_vae_locs = np.asarray(averaged_vae_locs)\n",
    "averaged_spike_times = np.asarray(averaged_spike_times)\n",
    "averaged_min_waveforms = np.asarray(averaged_min_waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our location estimates, we can plot them and visualize the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(channel_positions[:,1], channel_positions[:,2], marker='s', color='grey')\n",
    "plt.scatter(averaged_vae_locs[:,1], averaged_vae_locs[:,2], alpha=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to match the location estimates to ground truth, that is straightforward as well and is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_array = np.asarray(training_set)\n",
    "spike_ids_array = training_set_array[:,4]\n",
    "idx_sort = np.argsort(spike_ids_array)\n",
    "sorted_spike_ids_array = spike_ids_array[idx_sort]\n",
    "#Here we are finding duplicate spikes that will have location estimates averaged (amp jitter)\n",
    "vals, idx_start, count = np.unique(sorted_spike_ids_array, return_counts=True,\n",
    "                                return_index=True)\n",
    "res = np.split(idx_sort, idx_start[1:])\n",
    "#filter them with respect to their size, keeping only items occurring more than once\n",
    "vals = vals[count >= 1]\n",
    "res = filter(lambda x: x.size >= 1, res)\n",
    "dup_spike_ids = []\n",
    "for r in res:\n",
    "    dup_spike_ids.append(r)\n",
    "\n",
    "\n",
    "overlap = 'all'\n",
    "var_threshold = float(\"inf\")\n",
    "training_set_array = np.asarray(training_set)\n",
    "\n",
    "#all_vae_locs --> N x K where N is the number of neurons and K is the number of spikes for that neuron \n",
    "all_vae_locs, all_vae_variances, all_neuron_waveforms, all_spike_times = vae_functions.getEstimatedLocationsGroundTruth(model, dup_spike_ids, neuron_loc_array, neuron_array, spike_time_list, overlap_array, device, overlap, \\\n",
    "                                                                                                                        training_set_array, amp_threshold=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the location estimates colored by ground truth neuron identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "clustering_plotting_functions.plot2DLocations(channel_positions, range(50), all_vae_locs, soma_positions, dims=[1,2], alpha=.5, ax=ax, annotate=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the location estimate and model as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "\n",
    "waveform_size = waveforms_array.shape[2] - 1\n",
    "save_path = \"results/\"\n",
    "if(save):\n",
    "    f = open(save_path + \"loc_predictions_\"+str(width) + \"_\" + str(amp_jitter) + \"_amp_jit_wave_\"+str(waveform_size)+\".txt\",\"w+\")\n",
    "    for neuron in range(50):\n",
    "        for i, loc in enumerate(all_vae_locs[neuron]):\n",
    "            est_loc_string = str(loc[0]) + \",\" + str(loc[1]) + \",\" + str(loc[2]) + \",\"\n",
    "            spike_time = all_spike_times[neuron][i]\n",
    "            waveform = all_neuron_waveforms[neuron][i].numpy()\n",
    "            waveform_string = \",\".join([str(reading) for reading in waveform]) + \",\"\n",
    "            f.write(est_loc_string + waveform_string + str(spike_time) + ',' + str(neuron) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'exps_0': model.exps_0,\n",
    "                'exps': model.exps\n",
    "                }, save_path + \"model_state_dict_\"+str(width) + \"_\" + str(amp_jitter) + \"_amp_jit_wave_\"+str(waveform_size)+\".tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spikeinterface]",
   "language": "python",
   "name": "conda-env-spikeinterface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
